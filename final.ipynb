{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /opt/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_table(\"./MINDsmall_train/news.tsv\", header=None, names=[\n",
    "    \"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"\n",
    "])\n",
    "\n",
    "behaviors_df = pd.read_table(\"./MINDsmall_train/behaviors.tsv\", header=None, names=[\n",
    "    \"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News\n",
      "\n",
      "  news_id   category      subcategory  \\\n",
      "0  N55528  lifestyle  lifestyleroyals   \n",
      "1  N19639     health       weightloss   \n",
      "2  N61837       news        newsworld   \n",
      "3  N53526     health           voices   \n",
      "4  N38324     health          medical   \n",
      "\n",
      "                                               title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1                      50 Worst Habits For Belly Fat   \n",
      "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
      "4  How to Get Rid of Skin Tags, According to a De...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  These seemingly harmless habits are holding yo...   \n",
      "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "3  I felt like I was a fraud, and being an NBA wi...   \n",
      "4  They seem harmless, but there's a very good re...   \n",
      "\n",
      "                                             url  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
      "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
      "\n",
      "                                      title_entities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "2                                                 []   \n",
      "3                                                 []   \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
      "\n",
      "                                   abstract_entities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "3  [{\"Label\": \"National Basketball Association\", ...  \n",
      "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  \n",
      "Click behaviors\n",
      "   impression_id user_id                   time  \\\n",
      "0              1  U13740  11/11/2019 9:05:58 AM   \n",
      "1              2  U91836  11/12/2019 6:11:30 PM   \n",
      "2              3  U73700  11/14/2019 7:01:48 AM   \n",
      "3              4  U34670  11/11/2019 5:28:05 AM   \n",
      "4              5   U8125  11/12/2019 4:11:21 PM   \n",
      "\n",
      "                                             history  \\\n",
      "0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
      "1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
      "2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
      "3  N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
      "4                        N10078 N56514 N14904 N33740   \n",
      "\n",
      "                                         impressions  \n",
      "0                                  N55689-1 N35729-0  \n",
      "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
      "2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  \n",
      "3                N35729-0 N33632-0 N49685-1 N27581-0  \n",
      "4  N39985-0 N36050-0 N16096-0 N8400-1 N22407-0 N6...  \n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(subset=['title'], inplace=True)\n",
    "behaviors_df.dropna(subset=['impressions'], inplace=True)\n",
    "print(\"News\\n\")\n",
    "print(news_df.head())\n",
    "print(\"Click behaviors\")\n",
    "print(behaviors_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clicked_news(imp):\n",
    "    return [i.split('-')[0] for i in imp.split() if i.endswith('-1')]\n",
    "\n",
    "behaviors_df['clicked_news'] = behaviors_df['impressions'].apply(extract_clicked_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clicks = behaviors_df.explode('clicked_news')[['user_id', 'clicked_news']]\n",
    "user_clicks.columns = ['user_id', 'news_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: 50000, Unique news items: 7713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_clicks['user_idx'] = user_encoder.fit_transform(user_clicks['user_id'])\n",
    "user_clicks['item_idx'] = item_encoder.fit_transform(user_clicks['news_id'])\n",
    "\n",
    "num_users = user_clicks['user_idx'].nunique()\n",
    "num_items = user_clicks['item_idx'].nunique()\n",
    "print(f\"Unique users: {num_users}, Unique news items: {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 236344], num_nodes=57713, num_users=50000, num_items=7713)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Convert user and item indices to torch tensors\n",
    "edge_index = torch.tensor([\n",
    "    user_clicks['user_idx'].values,\n",
    "    user_clicks['item_idx'].values + num_users  # shift item indices to avoid overlap\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create PyG Data object for bipartite graph\n",
    "data = Data(edge_index=edge_index)\n",
    "\n",
    "# Save useful attributes for later use\n",
    "data.num_nodes = num_users + num_items\n",
    "data.num_users = num_users\n",
    "data.num_items = num_items\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, num_layers=3):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_nodes = num_users + num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Initialize user and item embeddings\n",
    "        self.embedding = nn.Embedding(self.num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        # Initial embeddings\n",
    "        x = self.embedding.weight\n",
    "\n",
    "        # To accumulate layer-wise embeddings\n",
    "        all_embeddings = [x]\n",
    "\n",
    "        for _ in range(self.num_layers):\n",
    "            # LightGCN propagation: simple mean aggregation from neighbors\n",
    "            row, col = edge_index\n",
    "            deg = torch.bincount(row, minlength=self.num_nodes).float().clamp(min=1)\n",
    "            norm = 1.0 / deg[row].sqrt() / deg[col].sqrt()\n",
    "            x = torch.zeros_like(x).scatter_add_(0, row.unsqueeze(-1).expand(-1, x.size(1)), x[col] * norm.unsqueeze(1))\n",
    "            all_embeddings.append(x)\n",
    "\n",
    "        # Final embedding is the sum of embeddings from all layers\n",
    "        out = torch.stack(all_embeddings, dim=0).mean(dim=0)\n",
    "        return out\n",
    "\n",
    "    def get_user_item_embeddings(self):\n",
    "        out = self.forward(edge_index)\n",
    "        user_emb = out[:self.num_users]\n",
    "        item_emb = out[self.num_users:]\n",
    "        return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def bpr_loss(user_emb, pos_emb, neg_emb):\n",
    "    pos_scores = torch.sum(user_emb * pos_emb, dim=1)\n",
    "    neg_scores = torch.sum(user_emb * neg_emb, dim=1)\n",
    "    loss = -F.logsigmoid(pos_scores - neg_scores).mean()\n",
    "    return loss\n",
    "\n",
    "def sample_mini_batch(edge_index, num_users, num_items, batch_size):\n",
    "    user_indices = torch.randint(0, num_users, (batch_size,))\n",
    "    pos_items = []\n",
    "    neg_items = []\n",
    "\n",
    "    for u in user_indices:\n",
    "        user_edges = edge_index[1][edge_index[0] == u]\n",
    "        if len(user_edges) == 0:\n",
    "            continue\n",
    "        pos = user_edges[random.randint(0, len(user_edges) - 1)]\n",
    "        while True:\n",
    "            neg = torch.randint(0, num_items, (1,)).item()\n",
    "            if neg + num_users not in user_edges:\n",
    "                break\n",
    "        pos_items.append(pos - num_users)\n",
    "        neg_items.append(neg)\n",
    "\n",
    "    return user_indices, torch.tensor(pos_items), torch.tensor(neg_items)\n",
    "\n",
    "# Set training params\n",
    "embedding_dim = 256\n",
    "num_layers = 3\n",
    "batch_size = 1024\n",
    "epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = LightGCN(num_users, num_items, embedding_dim, num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:31<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:31<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:32<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.5936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:33<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.4904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:35<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.3852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:32<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:32<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.2578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:37<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:39<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 0.2042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:39<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 0.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:38<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Loss: 0.1712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:35<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Loss: 0.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:38<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Loss: 0.1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:36<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Loss: 0.1485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:36<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Loss: 0.1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = data.num_users // batch_size + 1\n",
    "\n",
    "    for _ in trange(num_batches, desc=f\"Epoch {epoch+1}\"):\n",
    "        user_idx, pos_idx, neg_idx = sample_mini_batch(edge_index, num_users, num_items, batch_size)\n",
    "\n",
    "        user_emb, item_emb = model.get_user_item_embeddings()\n",
    "        u_emb = user_emb[user_idx]\n",
    "        pos_emb = item_emb[pos_idx]\n",
    "        neg_emb = item_emb[neg_idx]\n",
    "\n",
    "        loss = bpr_loss(u_emb, pos_emb, neg_emb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10 on validation set: 0.6784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Prepare a sample validation set (say, first 1000 impressions)\n",
    "val_behaviors = behaviors_df.head(1000).copy()\n",
    "val_behaviors = val_behaviors[val_behaviors['clicked_news'].map(len) > 0]\n",
    "\n",
    "def prepare_validation_samples(row):\n",
    "    clicked = [i for i in row['clicked_news']]\n",
    "    all_impressions = [i.split('-')[0] for i in row['impressions'].split()]\n",
    "    labels = [1 if news in clicked else 0 for news in all_impressions]\n",
    "    return pd.Series([row['user_id'], all_impressions, labels], index=['user_id', 'candidate_news', 'labels'])\n",
    "\n",
    "val_samples = val_behaviors.apply(prepare_validation_samples, axis=1)\n",
    "\n",
    "# Map news IDs to indices\n",
    "val_samples['user_idx'] = user_encoder.transform(val_samples['user_id'])\n",
    "val_samples['news_indices'] = val_samples['candidate_news'].apply(\n",
    "    lambda news_list: [item_encoder.transform([nid])[0] if nid in item_encoder.classes_ else -1 for nid in news_list]\n",
    ")\n",
    "\n",
    "# Validation function\n",
    "def evaluate_ndcg(model, val_samples, k=10):\n",
    "    model.eval()\n",
    "    user_emb, item_emb = model.get_user_item_embeddings()\n",
    "\n",
    "    ndcgs = []\n",
    "    for _, row in val_samples.iterrows():\n",
    "        user_idx = row['user_idx']\n",
    "        item_idxs = row['news_indices']\n",
    "        labels = row['labels']\n",
    "\n",
    "        # Skip samples with unknown news\n",
    "        if any(idx == -1 for idx in item_idxs):\n",
    "            continue\n",
    "\n",
    "        scores = torch.matmul(user_emb[user_idx], item_emb[item_idxs].T).detach().numpy()\n",
    "        ndcg = ndcg_score([labels], [scores], k=k)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return sum(ndcgs) / len(ndcgs) if ndcgs else 0.0\n",
    "\n",
    "# Example usage after training:\n",
    "ndcg10 = evaluate_ndcg(model, val_samples)\n",
    "print(f\"nDCG@10 on validation set: {ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"lightgcn_model.pth\")\n",
    "\n",
    "# Save embeddings\n",
    "torch.save(user_emb, \"user_embeddings.pt\")\n",
    "torch.save(item_emb, \"item_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "\n",
    "for uid in user_clicks['user_id'].unique():\n",
    "    uidx = user_encoder.transform([uid])[0]\n",
    "    scores = torch.matmul(user_emb[uidx], item_emb.T).detach().numpy()\n",
    "    topk_indices = scores.argsort()[::-1][:10]\n",
    "    topk_news_ids = item_encoder.inverse_transform(topk_indices)\n",
    "    recommendations.append({'user_id': uid, 'top_10_news': topk_news_ids.tolist()})\n",
    "\n",
    "import pandas as pd\n",
    "rec_df = pd.DataFrame(recommendations)\n",
    "rec_df.to_csv(\"user_recommendations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Recommendations for User ID: U1234\n",
      "\n",
      "1. Lamar Odom Is Engaged to Sabrina Parr: See Her Ring!\n",
      "2. Hannah Brown on Being Surrounded By Exes Tyler Cameron and Colton Underwood\n",
      "3. Celebrity plastic surgery transformations\n",
      "4. Stella McCartney Deleted a Meghan Markle Instagram Post After Followers Called Her Out\n",
      "5. 37 Years After His Wife Is Found Dead with an Ax in Her Skull, Husband Is Arrested\n",
      "6. College gymnast dies following training accident in Connecticut\n",
      "7. The son of a Chinese billionaire has been banned from flying first class, playing golf, buying property, or going clubbing\n",
      "8. Carrie Underwood Praises Miranda Lambert as 'Super Supportive': 'We Lift Each Other Up'\n",
      "9. Week in celebrity photos for Nov. 11-15, 2019\n",
      "10. Atlanta college student Alexis Crawford was choked to death, dumped in park, police say\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” Set your target user_id here\n",
    "target_user_id = \"U1234\"  # replace with any valid user_id from behaviors_df or user_clicks\n",
    "\n",
    "# Convert to internal user_idx\n",
    "if target_user_id not in user_encoder.classes_:\n",
    "    print(\"User ID not found in training data.\")\n",
    "else:\n",
    "    user_idx = user_encoder.transform([target_user_id])[0]\n",
    "\n",
    "    # Ensure model is in eval mode and get final embeddings\n",
    "    model.eval()\n",
    "    user_emb, item_emb = model.get_user_item_embeddings()\n",
    "\n",
    "    # Compute scores: dot product between user vector and all item vectors\n",
    "    scores = torch.matmul(user_emb[user_idx], item_emb.T).detach().numpy()\n",
    "\n",
    "    # Get top-k recommended item indices (e.g., top 10)\n",
    "    topk_indices = scores.argsort()[::-1][:10]\n",
    "\n",
    "    # Map back to news_id and titles\n",
    "    topk_news_ids = item_encoder.inverse_transform(topk_indices)\n",
    "    topk_titles = news_df.set_index('news_id').loc[topk_news_ids, 'title'].tolist()\n",
    "\n",
    "    # ðŸ“¢ Print Recommendations\n",
    "    print(f\"\\nTop 10 Recommendations for User ID: {target_user_id}\\n\")\n",
    "    for rank, title in enumerate(topk_titles, 1):\n",
    "        print(f\"{rank}. {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the models and embeddings\n",
    "user_emb = torch.load(\"user_embeddings.pt\")\n",
    "item_emb = torch.load(\"item_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightGCN(\n",
       "  (embedding): Embedding(57713, 256)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ðŸ” Rebuild the model architecture (must match the one used during training)\n",
    "model = LightGCN(num_users, num_items, embedding_dim=256, num_layers=3)\n",
    "\n",
    "# ðŸ”„ Load the trained weights\n",
    "model.load_state_dict(torch.load(\"lightgcn_model.pth\", map_location=torch.device('cpu')))\n",
    "\n",
    "# âœ… Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples retained: 335\n"
     ]
    }
   ],
   "source": [
    "# Load dev/test behaviors file\n",
    "dev_behaviors_path = \"MINDsmall_dev/behaviors.tsv\"\n",
    "\n",
    "# Load columns\n",
    "test_behaviors_df = pd.read_table(dev_behaviors_path, header=None, names=[\n",
    "    \"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"\n",
    "])\n",
    "\n",
    "# Drop impressions with no data\n",
    "test_behaviors_df.dropna(subset=[\"impressions\"], inplace=True)\n",
    "\n",
    "# Extract clicked news and prepare val-like samples\n",
    "def extract_clicked_news(impressions):\n",
    "    return [i.split('-')[0] for i in impressions.split() if i.endswith('-1')]\n",
    "\n",
    "test_behaviors_df['clicked_news'] = test_behaviors_df['impressions'].apply(extract_clicked_news)\n",
    "\n",
    "# Keep only valid samples (at least one click)\n",
    "test_behaviors = test_behaviors_df[test_behaviors_df['clicked_news'].map(len) > 0].copy()\n",
    "\n",
    "# Convert into format compatible with evaluate_ndcg()\n",
    "def prepare_test_sample(row):\n",
    "    clicked = row['clicked_news']\n",
    "    all_news = [i.split('-')[0] for i in row['impressions'].split()]\n",
    "    labels = [1 if n in clicked else 0 for n in all_news]\n",
    "    return pd.Series([row['user_id'], all_news, labels], index=['user_id', 'candidate_news', 'labels'])\n",
    "\n",
    "test_samples = test_behaviors.apply(prepare_test_sample, axis=1)\n",
    "\n",
    "# Map user and news IDs to internal indices\n",
    "test_samples['user_idx'] = test_samples['user_id'].apply(\n",
    "    lambda uid: user_encoder.transform([uid])[0] if uid in user_encoder.classes_ else -1\n",
    ")\n",
    "\n",
    "test_samples['news_indices'] = test_samples['candidate_news'].apply(\n",
    "    lambda news_list: [item_encoder.transform([nid])[0] if nid in item_encoder.classes_ else -1 for nid in news_list]\n",
    ")\n",
    "\n",
    "# Optional: remove samples with unknown users or items\n",
    "test_samples = test_samples[(test_samples['user_idx'] != -1) & \n",
    "                            (test_samples['news_indices'].apply(lambda x: -1 not in x))]\n",
    "\n",
    "print(f\"Test samples retained: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10 on MINDsmall_dev test set: 0.7913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_ndcg(model, val_samples, k=10):\n",
    "    model.eval()\n",
    "    user_emb, item_emb = model.get_user_item_embeddings()\n",
    "\n",
    "    ndcgs = []\n",
    "    for _, row in val_samples.iterrows():\n",
    "        user_idx = row['user_idx']\n",
    "        item_idxs = row['news_indices']\n",
    "        labels = row['labels']\n",
    "\n",
    "        # Skip samples with unknown news\n",
    "        if any(idx == -1 for idx in item_idxs):\n",
    "            continue\n",
    "\n",
    "        scores = torch.matmul(user_emb[user_idx], item_emb[item_idxs].T).detach().numpy()\n",
    "        ndcg = ndcg_score([labels], [scores], k=k)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return sum(ndcgs) / len(ndcgs) if ndcgs else 0.0\n",
    "    \n",
    "ndcg10_test = evaluate_ndcg(model, test_samples)\n",
    "print(f\"nDCG@10 on MINDsmall_dev test set: {ndcg10_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Embeddings Shape: torch.Size([51282, 5000])\n",
      "BERT Embeddings Shape: torch.Size([51282, 384])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('tfidf_vectorizer.pt', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "tfidf_embeddings = torch.load('tfidf_embeddings.pt')  # TF-IDF representation of news articles\n",
    "\n",
    "# 2. Load BERT Embeddings (semantic content embeddings)\n",
    "bert_embeddings = torch.load('bert_embeddings.pt')  # Precomputed BERT embeddings for news articles\n",
    "\n",
    "print(f\"TF-IDF Embeddings Shape: {tfidf_embeddings.shape}\")\n",
    "print(f\"BERT Embeddings Shape: {bert_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT4Rec User Embeddings Shape: torch.Size([51284, 64])\n",
      "BERT4Rec full model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 1. Load BERT4Rec user embeddings\n",
    "bert4rec_user_embeddings = np.load('bert4rec_embeddings.npy')  # shape: (num_users, embedding_dim)\n",
    "bert4rec_user_embeddings = torch.tensor(bert4rec_user_embeddings)\n",
    "\n",
    "print(f\"BERT4Rec User Embeddings Shape: {bert4rec_user_embeddings.shape}\")\n",
    "\n",
    "# 2. Load the full BERT4Rec model directly (NOT state_dict)\n",
    "bert4rec_model = torch.load('bert4rec_full_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# 3. Set to evaluation mode\n",
    "bert4rec_model.eval()\n",
    "\n",
    "print(\"BERT4Rec full model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gnn(user_id, user_encoder, item_encoder, model, edge_index=None):\n",
    "    \"\"\"\n",
    "    Computes GNN scores for a user.\n",
    "    \"\"\"\n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return None\n",
    "\n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "    user_emb, item_emb = model.get_user_item_embeddings()\n",
    "\n",
    "    scores = torch.matmul(user_emb[user_idx], item_emb.T).detach().numpy()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def score_tfidf(user_id, interactions_df, tfidf_embeddings, tfidf_vectorizer, user_encoder, news_id_to_idx):\n",
    "    \"\"\"\n",
    "    Computes TF-IDF scores for a user.\n",
    "    \"\"\"\n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return None\n",
    "\n",
    "    user_clicked = interactions_df[interactions_df['user_id'] == user_id]['news_id'].tolist()\n",
    "    clicked_idx = [news_id_to_idx.get(nid, None) for nid in user_clicked]\n",
    "    clicked_idx = [idx for idx in clicked_idx if idx is not None]\n",
    "\n",
    "    if not clicked_idx:\n",
    "        return np.zeros(tfidf_embeddings.shape[0])\n",
    "\n",
    "    user_vector = tfidf_embeddings[clicked_idx].mean(axis=0, keepdim=True)\n",
    "    similarities = cosine_similarity(user_vector, tfidf_embeddings).flatten()\n",
    "\n",
    "    return similarities\n",
    "\n",
    "def score_bert(user_id, interactions_df, bert_embeddings, user_encoder, news_id_to_idx):\n",
    "    \"\"\"\n",
    "    Computes BERT content similarity scores for a user.\n",
    "    \"\"\"\n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return None\n",
    "\n",
    "    user_clicked = interactions_df[interactions_df['user_id'] == user_id]['news_id'].tolist()\n",
    "    clicked_idx = [news_id_to_idx.get(nid, None) for nid in user_clicked]\n",
    "    clicked_idx = [idx for idx in clicked_idx if idx is not None]\n",
    "\n",
    "    if not clicked_idx:\n",
    "        return np.zeros(bert_embeddings.shape[0])\n",
    "\n",
    "    user_vector = bert_embeddings[clicked_idx].mean(dim=0)\n",
    "    scores = torch.matmul(user_vector, bert_embeddings.T).detach().numpy()\n",
    "\n",
    "    return scores\n",
    "\n",
    "def score_bert4rec(user_id, user_encoder, bert4rec_user_embeddings):\n",
    "    \"\"\"\n",
    "    Computes BERT4Rec user embedding scores.\n",
    "    \"\"\"\n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return None\n",
    "\n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    if user_idx >= bert4rec_user_embeddings.shape[0]:\n",
    "        return np.zeros(bert4rec_user_embeddings.shape[1])\n",
    "\n",
    "    user_emb = bert4rec_user_embeddings[user_idx]\n",
    "    scores = torch.matmul(user_emb, bert4rec_user_embeddings.T).detach().numpy()\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created all missing mappings: interactions_df, news_id_to_idx, idx_to_news_id, news_id_to_title.\n"
     ]
    }
   ],
   "source": [
    "# 1. interactions_df: user_id â†” news_id clicked\n",
    "try:\n",
    "    interactions_df\n",
    "except NameError:\n",
    "    interactions_df = user_clicks[['user_id', 'news_id']]\n",
    "\n",
    "# 2. news_id_to_idx and idx_to_news_id mappings\n",
    "try:\n",
    "    news_id_to_idx\n",
    "    idx_to_news_id\n",
    "except NameError:\n",
    "    news_id_to_idx = {nid: idx for idx, nid in enumerate(news_df['news_id'].tolist())}\n",
    "    idx_to_news_id = {idx: nid for nid, idx in news_id_to_idx.items()}\n",
    "\n",
    "# 3. news_id_to_title mapping\n",
    "try:\n",
    "    news_id_to_title\n",
    "except NameError:\n",
    "    news_id_to_title = dict(zip(news_df['news_id'], news_df['title']))\n",
    "\n",
    "print(\"âœ… Created all missing mappings: interactions_df, news_id_to_idx, idx_to_news_id, news_id_to_title.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def blended_recommendation(\n",
    "    user_id,\n",
    "    user_encoder,\n",
    "    item_encoder,\n",
    "    model,\n",
    "    edge_index,\n",
    "    interactions_df,\n",
    "    tfidf_embeddings,\n",
    "    tfidf_vectorizer,\n",
    "    bert_embeddings,\n",
    "    bert4rec_user_embeddings,\n",
    "    news_id_to_idx,\n",
    "    alpha=0.4,\n",
    "    beta=0.2,\n",
    "    gamma=0.2,\n",
    "    delta=0.2,\n",
    "    top_k=10,\n",
    "    candidate_news_ids=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate blended recommendations by scoring only on candidate news articles.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # --- Candidate news list ---\n",
    "    if candidate_news_ids is not None:\n",
    "        candidate_indices = [news_id_to_idx.get(nid, -1) for nid in candidate_news_ids if nid in news_id_to_idx]\n",
    "    else:\n",
    "        candidate_indices = list(range(tfidf_embeddings.shape[0]))\n",
    "\n",
    "    # --- GNN Scores ---\n",
    "    gnn_user_emb, gnn_item_emb = model.get_user_item_embeddings()\n",
    "    \n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return []\n",
    "\n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "\n",
    "    gnn_raw_scores = torch.matmul(gnn_user_emb[user_idx], gnn_item_emb.T).detach().cpu().numpy()\n",
    "\n",
    "    # Fill missing gnn scores\n",
    "    full_size = tfidf_embeddings.shape[0]\n",
    "    expanded_gnn_scores = np.full(full_size, -np.inf)\n",
    "    expanded_gnn_scores[:len(gnn_raw_scores)] = gnn_raw_scores\n",
    "\n",
    "    gnn_scores = expanded_gnn_scores[candidate_indices]\n",
    "\n",
    "    # --- TF-IDF Scores ---\n",
    "    tfidf_scores_full = score_tfidf(user_id, interactions_df, tfidf_embeddings, tfidf_vectorizer, user_encoder, news_id_to_idx)\n",
    "    tfidf_scores = tfidf_scores_full[candidate_indices]\n",
    "\n",
    "    # --- BERT Scores ---\n",
    "    bert_scores_full = score_bert(user_id, interactions_df, bert_embeddings, user_encoder, news_id_to_idx)\n",
    "    bert_scores = bert_scores_full[candidate_indices]\n",
    "\n",
    "    # --- BERT4Rec Scores ---\n",
    "    bert4rec_scores_full = score_bert4rec(user_id, user_encoder, bert4rec_user_embeddings)\n",
    "    bert4rec_scores = bert4rec_scores_full[candidate_indices]\n",
    "\n",
    "    def normalize(x):\n",
    "        x = np.nan_to_num(x)\n",
    "        if x.size == 0:\n",
    "            return x  # return empty safely\n",
    "        min_x = np.min(x)\n",
    "        max_x = np.max(x)\n",
    "        if max_x - min_x == 0:\n",
    "            return np.zeros_like(x)\n",
    "        return (x - min_x) / (max_x - min_x)\n",
    "\n",
    "    gnn_scores = normalize(gnn_scores)\n",
    "    tfidf_scores = normalize(tfidf_scores)\n",
    "    bert_scores = normalize(bert_scores)\n",
    "    bert4rec_scores = normalize(bert4rec_scores)\n",
    "\n",
    "    # --- Blend all normalized scores ---\n",
    "    final_scores = (\n",
    "        alpha * gnn_scores +\n",
    "        beta * tfidf_scores +\n",
    "        gamma * bert_scores +\n",
    "        delta * bert4rec_scores\n",
    "    )\n",
    "\n",
    "    # --- Pick top-k\n",
    "    top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
    "\n",
    "    if candidate_news_ids is not None:\n",
    "        top_news_ids = [candidate_news_ids[i] for i in top_indices]\n",
    "    else:\n",
    "        top_news_ids = item_encoder.inverse_transform(top_indices)\n",
    "\n",
    "    return top_news_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_user(\n",
    "    user_id,\n",
    "    user_encoder,\n",
    "    item_encoder,\n",
    "    model,\n",
    "    edge_index,\n",
    "    interactions_df,\n",
    "    tfidf_embeddings,\n",
    "    tfidf_vectorizer,\n",
    "    bert_embeddings,\n",
    "    bert4rec_user_embeddings,\n",
    "    news_id_to_idx,\n",
    "    top_k=10,\n",
    "    alpha=0.4,\n",
    "    beta=0.2,\n",
    "    gamma=0.2,\n",
    "    delta=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict Top-K news articles for a given user using the blended model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call blended_recommendation (predict across all news if no candidate list)\n",
    "    top_news_ids = blended_recommendation(\n",
    "        user_id=user_id,\n",
    "        user_encoder=user_encoder,\n",
    "        item_encoder=item_encoder,\n",
    "        model=model,\n",
    "        edge_index=edge_index,\n",
    "        interactions_df=interactions_df,\n",
    "        tfidf_embeddings=tfidf_embeddings,\n",
    "        tfidf_vectorizer=tfidf_vectorizer,\n",
    "        bert_embeddings=bert_embeddings,\n",
    "        bert4rec_user_embeddings=bert4rec_user_embeddings,\n",
    "        news_id_to_idx=news_id_to_idx,\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        gamma=gamma,\n",
    "        delta=delta,\n",
    "        top_k=top_k,\n",
    "        candidate_news_ids=None  # Predict across all news if not specified\n",
    "    )\n",
    "\n",
    "    return top_news_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Recommended News for User U12345:\n",
      "1. Texas Plane Crashed After Dropping Water For Gender Reveal: NTSB\n",
      "2. 7 Rules for How to Call in Sick to Work\n",
      "3. Some Burning Steelers Questions about Mike Tomlin\n",
      "4. Rudy Giuliani's globetrotting complicates US foreign policy\n",
      "5. No 'Ralphie' for a second straight game leaves CU fans concerned\n",
      "6. Coast Guard suspends search for missing airman\n",
      "7. Watch: Fans go crazy when Burrow arrives back in Baton Rouge after win over Tide\n",
      "8. Fear spreads among Iraqi protesters as government cracks down, keeps death toll secret\n",
      "9. Fastest American Muscle Cars EVER\n",
      "10. Walmart sales hit all-time high\n"
     ]
    }
   ],
   "source": [
    "target_user_id = \"U12345\"  # Replace with a valid user ID\n",
    "predicted_top_news = predict_for_user(\n",
    "    user_id=target_user_id,\n",
    "    user_encoder=user_encoder,\n",
    "    item_encoder=item_encoder,\n",
    "    model=model,\n",
    "    edge_index=edge_index,\n",
    "    interactions_df=interactions_df,\n",
    "    tfidf_embeddings=tfidf_embeddings,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    bert_embeddings=bert_embeddings,\n",
    "    bert4rec_user_embeddings=bert4rec_user_embeddings,\n",
    "    news_id_to_idx=news_id_to_idx,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(f\"Top-10 Recommended News for User {target_user_id}:\")\n",
    "for idx, news_id in enumerate(predicted_top_news, 1):\n",
    "    title = news_id_to_title.get(news_id, \"Unknown Title\")\n",
    "    print(f\"{idx}. {title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final corrected test_samples created: 335 user sessions.\n"
     ]
    }
   ],
   "source": [
    "# --- Load behaviors.tsv if not already ---\n",
    "behaviors_df = pd.read_csv('MINDsmall_dev/behaviors.tsv', sep='\\t', header=None,\n",
    "                           names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "# --- Drop rows with no impressions ---\n",
    "behaviors_df.dropna(subset=['impressions'], inplace=True)\n",
    "\n",
    "# --- Extract clicked news from impressions (correct way) ---\n",
    "def extract_clicked_news(impressions):\n",
    "    return [imp.split('-')[0] for imp in impressions.split() if imp.endswith('-1')]\n",
    "\n",
    "behaviors_df['clicked_news'] = behaviors_df['impressions'].apply(extract_clicked_news)\n",
    "\n",
    "# --- Filter sessions with at least one click ---\n",
    "test_behaviors = behaviors_df[behaviors_df['clicked_news'].map(len) > 0].copy()\n",
    "\n",
    "# --- Prepare test_samples exactly like GNN evaluation ---\n",
    "def prepare_test_sample(row):\n",
    "    clicked = row['clicked_news']\n",
    "    all_news = [imp.split('-')[0] for imp in row['impressions'].split()]\n",
    "    labels = [1 if n in clicked else 0 for n in all_news]\n",
    "    return pd.Series([row['user_id'], all_news, labels], index=['user_id', 'candidate_news', 'labels'])\n",
    "\n",
    "test_samples = test_behaviors.apply(prepare_test_sample, axis=1)\n",
    "\n",
    "# --- Filter only seen users ---\n",
    "test_samples = test_samples[test_samples['user_id'].isin(user_encoder.classes_)].copy()\n",
    "\n",
    "# --- Map to user indices and news indices ---\n",
    "test_samples['user_idx'] = user_encoder.transform(test_samples['user_id'])\n",
    "\n",
    "test_samples['news_indices'] = test_samples['candidate_news'].apply(\n",
    "    lambda news_list: [item_encoder.transform([nid])[0] if nid in item_encoder.classes_ else -1 for nid in news_list]\n",
    ")\n",
    "\n",
    "# --- Drop samples with -1 indices (unknown news) ---\n",
    "test_samples = test_samples[(test_samples['news_indices'].apply(lambda x: all(idx != -1 for idx in x)))]\n",
    "\n",
    "print(f\"âœ… Final corrected test_samples created: {len(test_samples)} user sessions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@10 for Blended Model: 0.7388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_blended_model(\n",
    "    model,\n",
    "    val_samples,\n",
    "    user_encoder,\n",
    "    item_encoder,\n",
    "    edge_index,\n",
    "    interactions_df,\n",
    "    tfidf_embeddings,\n",
    "    tfidf_vectorizer,\n",
    "    bert_embeddings,\n",
    "    bert4rec_user_embeddings,\n",
    "    news_id_to_idx,\n",
    "    k=10,\n",
    "    alpha=0.4,\n",
    "    beta=0.2,\n",
    "    gamma=0.2,\n",
    "    delta=0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the blended model using only the candidate impressions shown to user.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ndcgs = []\n",
    "\n",
    "    for _, row in val_samples.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        candidate_news_ids = row['candidate_news']\n",
    "        labels = row['labels']\n",
    "\n",
    "        # --- Pass candidate_news_ids correctly ---\n",
    "        predicted_top_news = blended_recommendation(\n",
    "            user_id=user_id,\n",
    "            user_encoder=user_encoder,\n",
    "            item_encoder=item_encoder,\n",
    "            model=model,\n",
    "            edge_index=edge_index,\n",
    "            interactions_df=interactions_df,\n",
    "            tfidf_embeddings=tfidf_embeddings,\n",
    "            tfidf_vectorizer=tfidf_vectorizer,\n",
    "            bert_embeddings=bert_embeddings,\n",
    "            bert4rec_user_embeddings=bert4rec_user_embeddings,\n",
    "            news_id_to_idx=news_id_to_idx,\n",
    "            top_k=len(candidate_news_ids),\n",
    "            candidate_news_ids=candidate_news_ids,  # IMPORTANT!!\n",
    "            alpha=alpha,\n",
    "            beta=beta,\n",
    "            gamma=gamma,\n",
    "            delta=delta\n",
    "        )\n",
    "\n",
    "        # Create binary relevance: 1 if predicted in top news, else 0\n",
    "        predicted_binary = [1 if nid in predicted_top_news else 0 for nid in candidate_news_ids]\n",
    "\n",
    "        if len(predicted_binary) != len(labels):\n",
    "            continue  # skip if mismatch\n",
    "\n",
    "        ndcg = ndcg_score([labels], [predicted_binary], k=k)\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return sum(ndcgs) / len(ndcgs) if ndcgs else 0.0\n",
    "    \n",
    "# --- Now evaluate ---\n",
    "final_ndcg10 = evaluate_blended_model(\n",
    "    model=model,\n",
    "    val_samples=test_samples,\n",
    "    user_encoder=user_encoder,\n",
    "    item_encoder=item_encoder,\n",
    "    edge_index=edge_index,\n",
    "    interactions_df=interactions_df,\n",
    "    tfidf_embeddings=tfidf_embeddings,\n",
    "    tfidf_vectorizer=tfidf_vectorizer,\n",
    "    bert_embeddings=bert_embeddings,\n",
    "    bert4rec_user_embeddings=bert4rec_user_embeddings,\n",
    "    news_id_to_idx=news_id_to_idx,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "print(f\"nDCG@10 for Blended Model: {final_ndcg10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
